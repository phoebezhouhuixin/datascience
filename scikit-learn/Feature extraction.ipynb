{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2-final"
    },
    "colab": {
      "name": "Feature extraction.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrlPAGgdrxcO",
        "colab_type": "text"
      },
      "source": [
        "# Feature extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN2bt3dkrxcS",
        "colab_type": "text"
      },
      "source": [
        "## Extracting features from categorical variables\n",
        "\n",
        "It may seem intuitive to represent the values with a single integer feature. But it encodes artifical information. There is no natural order of cities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slEhVhmIrxcS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "ea66f995-1961-40a4-f8bd-6dfa07c42e13"
      },
      "source": [
        "from sklearn.feature_extraction import DictVectorizer\n",
        "\n",
        "onehot_encoder = DictVectorizer()\n",
        "X = [\n",
        "    {'city': 'New York'},\n",
        "    {'city': 'San Francisco'},\n",
        "    {'city': 'Chapel Hill'}\n",
        "]\n",
        "\n",
        "print(onehot_encoder.fit_transform(X).toarray())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0jQwdjBrxca",
        "colab_type": "text"
      },
      "source": [
        "## Standardizing features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3Yn5BBorxcb",
        "colab_type": "text"
      },
      "source": [
        "The standardized data has zero mean and unit variance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmoda-7Xrxcb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "cc1d6a4c-6587-41d4-8245-c53ca09952b6"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([\n",
        "    [0., 0., 5., 13., 9., 1.],\n",
        "    [0., 0., 13., 15., 10., 15.],\n",
        "    [0., 3., 15., 2., 0., 11.]\n",
        "])\n",
        "\n",
        "print(preprocessing.scale(X))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.         -0.70710678 -1.38873015  0.52489066  0.59299945 -1.35873244]\n",
            " [ 0.         -0.70710678  0.46291005  0.87481777  0.81537425  1.01904933]\n",
            " [ 0.          1.41421356  0.9258201  -1.39970842 -1.4083737   0.33968311]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3n6bYELrxcf",
        "colab_type": "text"
      },
      "source": [
        "To mitigate the effect of large outliers use `RobustScaler`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fs4HM3Arxcg",
        "colab_type": "text"
      },
      "source": [
        "## Extracting features from text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRxsgi4Frxch",
        "colab_type": "text"
      },
      "source": [
        "### The bag-of-words model\n",
        "\n",
        "The bag-of-words model is motivated by the intuition that documents containing similar words often have similar meaning.  A collection of documents is called a **corpus**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PLdqNQ-rxci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = [\n",
        "    'UNC played Duke in basketball',\n",
        "    'Duke lost the basketball game',\n",
        "    'I ate a sandwich', \n",
        "    'playing playly playful player '\n",
        "]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-I4gWJzvrxcl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RszTOuY9rxcn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "c4b15f54-6110-4015-b2f9-f98f8f92bf5d"
      },
      "source": [
        "print(vectorizer.fit_transform(corpus).todense())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 1 0 1 0 1 0 0 0 0 0 0 1]\n",
            " [0 1 1 1 0 1 0 0 0 0 0 0 1 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0 1 1 1 1 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIBWOK7Orxcs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b9d2f1b1-f3d0-4d4a-c065-716411af010b"
      },
      "source": [
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'unc': 13, 'played': 6, 'duke': 2, 'in': 4, 'basketball': 1, 'lost': 5, 'the': 12, 'game': 3, 'ate': 0, 'sandwich': 11, 'playing': 9, 'playly': 10, 'playful': 8, 'player': 7}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xQ2Wan6rxcv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "e43e3086-24d5-4936-a35d-c863de190e01"
      },
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "X = vectorizer.fit_transform(corpus).todense()\n",
        "print('Distance between 1st and 2nd documents: ', euclidean_distances(X[0], X[1]))\n",
        "print('Distance between 1st and 3rd documents: ', euclidean_distances(X[0], X[2]))\n",
        "print('Distance between 2nd and 3rd documents: ', euclidean_distances(X[1], X[2]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Distance between 1st and 2nd documents:  [[2.44948974]]\n",
            "Distance between 1st and 3rd documents:  [[2.64575131]]\n",
            "Distance between 2nd and 3rd documents:  [[2.64575131]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us8peAwCrxc-",
        "colab_type": "text"
      },
      "source": [
        "#### Stop word filtering\n",
        "\n",
        "A basic strategy for the reducing dimanesions is to convert all of the text to lowercase. A second strategy is to remove words that are common to most of the documents:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_03fP3Orxc-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "3f62d5b5-eee5-447c-fe74-897fc02a2857"
      },
      "source": [
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "print(vectorizer.fit_transform(corpus).todense())\n",
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 1 0 0 1 0 0 0 0 0 1]\n",
            " [0 1 1 1 1 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 1 1 1 1 0 0]]\n",
            "{'unc': 11, 'played': 5, 'duke': 2, 'basketball': 1, 'lost': 4, 'game': 3, 'ate': 0, 'sandwich': 10, 'playing': 8, 'playly': 9, 'playful': 7, 'player': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ES9kvP9rxdF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "0e9f2c4c-4452-45e5-d8f0-829f08443856"
      },
      "source": [
        "corpus = [\n",
        "    'He ate the sandwiches',\n",
        "    'Every sandwich was eaten by him'\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
        "print(vectorizer.fit_transform(corpus).todense())\n",
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 0 0 1]\n",
            " [0 1 1 0]]\n",
            "{'ate': 0, 'sandwiches': 3, 'sandwich': 2, 'eaten': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQMOGheXrxdO",
        "colab_type": "text"
      },
      "source": [
        "The documents have similar meaning, but their feature vectors have no elements in common!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPZLgbFBrxdN",
        "colab_type": "text"
      },
      "source": [
        "### Stemming and lemmatization\n",
        "\n",
        "Stemming and lemmatization are two different strategies for condensing derived forms of word into a single root word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B91ogOhfxY6v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "efee518a-0ac4-4815-9664-10f52f4b1836"
      },
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-yVvTW9rxdO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "fd9de1cb-2b2e-4dfe-f3d3-5e817afb55f8"
      },
      "source": [
        "corpus = [\n",
        "    'He ate the sandwiches',\n",
        "    'Every sandwich was eaten by him'\n",
        "]\n",
        "[[token for token in word_tokenize(document)] for document in corpus]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['He', 'ate', 'the', 'sandwiches'],\n",
              " ['Every', 'sandwich', 'was', 'eaten', 'by', 'him']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8MoMDDhrxdV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ps = PorterStemmer()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cK03y2J8rxdX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d7c60e30-f43d-42e6-d562-20f5c1f57769"
      },
      "source": [
        "for document in corpus:\n",
        "    print(ps.stem(document))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "he ate the sandwich\n",
            "every sandwich was eaten by him\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2qIogGkrxdb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "714f0544-b3a0-41aa-8b28-115fc781e5ff"
      },
      "source": [
        "corpus"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['He ate the sandwiches', 'Every sandwich was eaten by him']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZnsoC58rxde",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "outputId": "3851b3f7-8de7-4a5b-ee01-3f65c0f112e9"
      },
      "source": [
        "# another example\n",
        "mystr = \"give giving gives gave given giver\"\n",
        "for word in mystr:\n",
        "    print(ps.stem(word)) # oops no"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "g\n",
            "i\n",
            "v\n",
            "e\n",
            " \n",
            "g\n",
            "i\n",
            "v\n",
            "i\n",
            "n\n",
            "g\n",
            " \n",
            "g\n",
            "i\n",
            "v\n",
            "e\n",
            "s\n",
            " \n",
            "g\n",
            "a\n",
            "v\n",
            "e\n",
            " \n",
            "g\n",
            "i\n",
            "v\n",
            "e\n",
            "n\n",
            " \n",
            "g\n",
            "i\n",
            "v\n",
            "e\n",
            "r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDxrSoPOrxdm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "561f34bf-5cda-4ff1-e147-20d95f7a5d10"
      },
      "source": [
        "tokenized_mystr = word_tokenize(mystr)\n",
        "tokenized_mystr"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['give', 'giving', 'gives', 'gave', 'given', 'giver']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGmWW8kxrxdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr04uI_zrxdw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "e765f954-d356-4473-adff-12082a2587af"
      },
      "source": [
        "for word in tokenized_mystr:\n",
        "    print(ps.stem(word))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "give\n",
            "give\n",
            "give\n",
            "gave\n",
            "given\n",
            "giver\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qa0iBlBnrxdz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e3be8d53-8b1b-475a-9300-7b90a9c06dcc"
      },
      "source": [
        "wordnet_tag = ['n', 'v']\n",
        "stemmer = PorterStemmer()\n",
        "print('Stemmed: ', [[stemmer.stem(token) for token in word_tokenize(document)] for document in corpus])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stemmed:  [['He', 'ate', 'the', 'sandwich'], ['everi', 'sandwich', 'wa', 'eaten', 'by', 'him']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHTbSZE_yCDc",
        "colab_type": "text"
      },
      "source": [
        "#### Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zadMZp0rxd3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize(token, tag):\n",
        "        if tag[0].lower() in ['n', 'v']:  # if the first letter of the tag indicates noun, verb (i.e. parts-of-speech tagging)\n",
        "            return lemmatizer.lemmatize(token, tag[0].lower())\n",
        "        return token"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHI55MQYrxd6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "c5505a29-17e0-49de-9b1a-e17cc117e4bb"
      },
      "source": [
        "tagged_corpus = pos_tag(word_tokenize(\"Today is going to be wholesome and filled with happiness\"))\n",
        "tagged_corpus"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Today', 'NN'),\n",
              " ('is', 'VBZ'),\n",
              " ('going', 'VBG'),\n",
              " ('to', 'TO'),\n",
              " ('be', 'VB'),\n",
              " ('wholesome', 'JJ'),\n",
              " ('and', 'CC'),\n",
              " ('filled', 'VBN'),\n",
              " ('with', 'IN'),\n",
              " ('happiness', 'NN')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adqtrUeWrxd8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0d4dd6cd-33ef-4fe9-c8bd-f4f343189bd8"
      },
      "source": [
        "for word, pos in tagged_corpus:\n",
        "    print(lemmatize(word, pos), end=\" \")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Today be go to be wholesome and fill with happiness "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEZT_zRnrxd_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "6966a273-1916-4ba3-cdbb-0ce44a1a139b"
      },
      "source": [
        "# another example\n",
        "corpus = [\n",
        "    'He ate the sandwiches',\n",
        "    'Every sandwich was eaten by him'\n",
        "]\n",
        "lemmatizer =WordNetLemmatizer()\n",
        "tagged_corpus = [pos_tag(word_tokenize(document)) for document in corpus]\n",
        "print(tagged_corpus)\n",
        "print('Lemmatized: ', [[lemmatize(token, tag) for token, tag in document] for document in tagged_corpus])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[('He', 'PRP'), ('ate', 'VBD'), ('the', 'DT'), ('sandwiches', 'NNS')], [('Every', 'DT'), ('sandwich', 'NN'), ('was', 'VBD'), ('eaten', 'VBN'), ('by', 'IN'), ('him', 'PRP')]]\n",
            "Lemmatized:  [['He', 'eat', 'the', 'sandwich'], ['Every', 'sandwich', 'be', 'eat', 'by', 'him']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rW9ATn1PrxeP",
        "colab_type": "text"
      },
      "source": [
        "### Extending bag-of-words with tf-idf weights\n",
        "\n",
        "Our feature vectors do not encode grammar, word order, or frequencies of words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJyUIi9FrxeP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "b7c99b27-0d46-488c-a49d-1091593f427c"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "corpus = [\n",
        "    'The dog ate a sandwich and I ate a sandwich',\n",
        "    'The wizard crafted a sandwich'\n",
        "]\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "print(vectorizer.fit_transform(corpus).todense())\n",
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.75458397 0.         0.37729199 0.53689271 0.        ]\n",
            " [0.         0.6316672  0.         0.44943642 0.6316672 ]]\n",
            "{'dog': 2, 'ate': 0, 'sandwich': 3, 'wizard': 4, 'crafted': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_2ZS_8_rxeS",
        "colab_type": "text"
      },
      "source": [
        "### Space-efficient feature vectorizing with the hashing trick\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiGIWWUxrxeT",
        "colab_type": "code",
        "colab": {},
        "outputId": "7e303897-5f02-446a-8711-1b0f9ec51e7e"
      },
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "\n",
        "corpus = ['the', 'ate', 'bacon', 'cat']\n",
        "vectorizer = HashingVectorizer(n_features=6)\n",
        "print(vectorizer.fit_transform(corpus).todense())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-1.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  1.  0.  0.]\n",
            " [ 0.  0.  0.  0. -1.  0.]\n",
            " [ 0.  1.  0.  0.  0.  0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCeJ33DcrxeW",
        "colab_type": "text"
      },
      "source": [
        "## Word embeddings\n",
        "\n",
        "While the bag-of-words model uses a scalar to represent each token, word embeddings use a vector. Words that are semantically similar to each other a represented by vectors are near each other. Concretely, words embedding are parametrized functions that take a token from some language as an input and output a vector. This function is essentially a lookup table that is parametrized by a matrix of embeddings.\n",
        "\n",
        "    The second component is a binary classifier that predicts whether the five vectors represent a valid se"
      ]
    }
  ]
}