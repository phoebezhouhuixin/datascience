# Ensemble Learning

**What?**

Ensemble learning refers to a collection of methods that learn a target function by training a number of individual learners and combining their predictions.

**Why?**

- Accuracy: a more reliable mapping can be obtained by combining the output of multiple *experts* (i.e. classifiers).
- Efficiency: a complex problem can be decomposed into multiple sub-problems that are easier to understand and solve (divide-and-conquer approach).
- Multiple Representations & Multiple Models: There is not a single model that works for all pattern recognition problems!

**When?**

When it is possible to build component classifiers that are more accurate than chance and, more importantly, that are independent from each other.

![Ensemble Learning](img/Ensemble%20Learning.png)

## Why do ensembles work?

### Increased Accuracy

If the error of individual classifiers is independent, we can eliminate the error of the combined classifier.

Assume a binary classification problem for which individual classifiers can be trained with an error rate of 0.3. Assume that an ensemble is built by combining the prediction of 21 such classifiers with a majority vote. If there are $i$ classifiers out of 21 making an error in prediction, the error of classification is:

$$
\begin{gathered}
\begin{pmatrix} 21 \\ i \end{pmatrix} 0.3^i (1 - 0.3)^{21 - i} \\
\text{where} \, \begin{pmatrix} 21 \\ i \end{pmatrix} = \frac{21!}{(21 - i)! \, i!}
\end{gathered}
$$

In order for the ensemble to misclassify an example, 11 or more classifiers have to be in error, or a probability of 0.026.

$$
\sum_{i = 11}^{21} \begin{pmatrix} 21 \\ i \end{pmatrix} 0.3^i (1 - 0.3)^{21 - i} = 0.026
$$

In general, the probability of an ensemble of $L$ classifiers with an accuracy rate of $p > 0.5$ correctly classifying an example is:

$$
\sum_{i = \big\lfloor \frac{L}{2} \big\rfloor + 1}^L \begin{pmatrix} L \\ i \end{pmatrix} p^i (1 - p)^{L - 1}
$$

### Approximation by Ensemble Averaging

The desired target function may not be implementable with individual classifiers, but may be approximated by ensemble averaging. For example, if many linear discriminant functions are combined, the true non-linear boundary can be approximated.

## Methods for Constructing Ensembles

1. Subsampling the training examples
    a. Multiple hypotheses are generated by training individual classifiers on different datasets obtained by resampling a common training set (Bagging, Boosting).
2. Manipulating the input features
    a. Multiple hypotheses are generated by training individual classifiers on different representations or different subsets of a common feature vector.
3. Manipulating the output targets
    a. The output targets for C classes are encoded with an L-bit codeword and an individual classifier is built to predict each one of the bits in the codeword.
    b. Additional "auxiliary" targets may be used to differentiate classifiers.
4. Modifying the learning parameters of the classifier
    a. A number of classifiers are built with different learning parameters, such as the number of neighbors in a $k$-Nearest Neighbor rule, initial weights in a Multi-Layer Perceptron etc.

## Structure of Ensemble Classifiers

### Parallel

All the individual classifiers are invoked independently and their results are fused with a combination rule (e.g. average, weighted voting) or a meta-classifier (e.g. stacked generalization).

![Parallel Ensemble Learning](img/Parallel%20EL.png)

### Cascading or Hierarchical

Classifiers are invoked in a sequential or tree-structured fashion. For the purpose of efficiency, inaccurate but fast methods are invoked first (maybe using a small subset of the features) and computationally more intensive but more accurate methods are left for the latter stages.

![Cascading Ensemble Learning](img/Cascading%20EL.png)

![Hierarchical Ensemble Learning](img/Hierarchical%20EL.png)

## Combination Strategies

### Static Combiners

In static combiners, the combiner decision rule is independent of the feature vector. Static approaches can be broadly divided into non-trainable and trainable.

- Non-trainable: The voting is performed independent of the performance of each individual classifier.
    - Voting: used when each classifier produces a single class label. In this case, each classifier "votes" for a particular class and the class with the majority vote on the ensemble wins.
    - Averaging: used when each classifier produces a confidence estimate (e.g. a posterior). In this case, the winner is the class with the highest average posterior across the ensemble.
- Trainable: The combiner undergoes a separate training phase to improve the performance of the ensemble machine. Two noteworthy approaches are:
    - Weighted Averaging: the output of each classifier is weighted by a measure of its own performance, for example, prediction accuracy on a separate validation set.
    - Stacked Generalization: the output of the ensemble serves as a feature vector to a meta-classifier.

#### Stacked Generalization

In stacked generalization, the output pattern of an ensemble of trained experts serves as an input to a second-level expert.

Training of this modular ensemble can be performed as follows:

1. From a dataset $X$ with $N$ examples, leave out one test example and train each of the Level-0 experts on the remaining $(N - 1)$ examples.
2. Generate a prediction for the test example. The output pattern $y = [y_1, y_2, \hdots y_K]$ across the Level-0 experts, along with the target $t$ for the test example, becomes a training example for the Level-1 expert.
3. Repeat this process in a leave-one-out fashion. This yields a training set $Y$ with $N$ examples, which is used to train the Level-1 expert separately.
4. To make full use of the training data, re-train all the Level-0 experts one more time using all $N$ examples in $X$.

![Stacked Generalization](img/Stacked%20Generalization%20(Ensemble%20Learning).png)

### Adaptive Combiners

An adaptive combiner is a function that depends on the input feature vector. Thus, the ensemble implements a function that is local to each region in the feature space.

This divide-and-conquer approach leads to modular ensembles where relatively simple classifiers specialize in different parts of the input-output space. In contrast with static combiner ensembles, the individual experts here do not need to perform well for all inputs, only in their region of expertise.

Representative examples of this approach are Mixture of Experts (ME) and Hierarchical ME.

![Adaptive Learning](img/Adaptive%20Learning%20(Ensemble%20Learning).png)

#### Mixture of Experts

- A gating network is used to partition the feature space into different regions, with one expert in the ensemble being responsible for generating the correct output within one region.
- The experts in the ensemble and the gating network are trained simultaneously, which can be efficiently performed with the expectation-maximization algorithm.
- ME can be extended to a multi-level hierarchical structure, where each component is itself a ME. In this case, a linear network can be used for the terminal classifiers without compromising the modeling capabilities of the machine.

![Mixture of Experts](img/Mixture%20of%20Experts%20(Ensemble%20Learning).png)

## Bagging

"Bagging" (bootstrap aggregation) is one of the simplest examples of "arcing" (adaptive re-weighting and combining).

**Algorithm:**

1. Given $N$ labeled data points $\{x_i\}$, use random sampling (with replacement) to create $K$ new data sets, each with $Nâ€™ < N$ data points.
2. Learn a classifier (of whatever type) based on each of the $K$ data sets giving $K$ classifiers.
3. Classify a new point $x$ based on a majority vote among the $K$ classifiers.

- The perturbation in the training set due to the bootstrap resampling causes different hypotheses to be built, particularly if the classifier is unstable.
    - A classifier is said to be unstable if a small change in the training data (e.g. order of presentation of examples) can lead to a radically different hypothesis. This is the case of decision trees and (arguably) neural networks.
- Bagging can be expected to improve accuracy if the induced classifiers are uncorrelated.
    - In some cases, such as $k$-Nearest Neighbors, bagging has been shown to degrade performance as compared to individual classifiers as a result of an effectively smaller training set.

## Boosting

Boosting takes a different resampling approach than bagging, which maintains a constant probability of $1 / N$ for selecting each individual example. In boosting, this probability is adapted over time based on performance. The component classifiers are built sequentially and examples that are mislabeled by previous components are chosen more often than those that are correctly classified.

Boosting is based on the concept of a "weak learner" (i.e. an algorithm that performs slightly better than chance, for e.g., a binary classifier with a >50% classification rate). A weak learner can be converted into a strong learner by changing the distribution of the training examples. While boosting can also be used with classifiers that are highly accurate, the benefits in this case will be very small.

A popular variant of boosting is AdaBoost (Adaptive Boosting), which allows the designer to continue adding components until an arbitrarily small error rate is obtained on the training set.

![Illustration of Re-Weighting Concept](img/Illustration%20of%20Re-Weighting%20Concept.png)

\vfill\eject

### AdaBoost

![AdaBoost Algorithm](img/AdaBoost%20Algorithm.png)
